A curated list of foundation models include: 
 + Large Language Models
 + Multimodal foundation models
 + Embodied foundation models


## 2025
[Meta] Llama 4. [Blog](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)

## 2024
[Meta] Llama 3. The Llama 3 Herd of Models. [Paper](https://arxiv.org/pdf/2407.21783)

## 2023
[Meta][Jul] Llama 2. Llama 2: Open Foundation and Fine-Tuned Chat Models [Paper](https://arxiv.org/pdf/2307.09288)
[Meta][Feb] Llama 1. LLaMA: Open and Efficient Foundation Language Models [Paper](https://arxiv.org/pdf/2302.13971)
[OpenAI][GPT-4] GPT-4 Technical Report. [Paper](https://arxiv.org/abs/2303.08774). [Code not published]

## 2020 
[OpenAI][GPT-3] Language Models are Few-Shot Learners. [Paper](https://arxiv.org/abs/2005.14165). [Code](https://github.com/openai/gpt-3)

## 2019
[OpenAI][GPT-2] Language Models are Unsupervised Multitask Learners. [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). [Code](https://github.com/openai/gpt-2)

## 2018
[Google][BERT] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. [Paper](file:///Users/mhuynh/Downloads/1810.04805v2.pdf)
  - Bi-directional transformer-based model.
  - The core model of pre-training and fine-tuning model are the same.
  - Model are similar to GPT-1. Bert-Base 110M params (same size with GPT-1). BERT-Large is 340M params.
  - Pre-training Tasks: Mask Language Model (MLM), Next Sequence Prediction.
  - Fine-tuning Tasks: 
    
[OpenAI][GPT-1]Improving Language Understanding by Generative Pre-Training. [Paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

# Datasets
